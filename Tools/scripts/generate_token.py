#! /usr/bin/env python3
# This script generates token related files kutoka Grammar/Tokens:
#
#   Doc/library/token-list.inc
#   Include/token.h
#   Parser/token.c
#   Lib/token.py


NT_OFFSET = 256

eleza load_tokens(path):
    tok_names = []
    string_to_tok = {}
    ERRORTOKEN = Tupu
    ukijumuisha open(path) kama fp:
        kila line kwenye fp:
            line = line.strip()
            # strip comments
            i = line.find('#')
            ikiwa i >= 0:
                line = line[:i].strip()
            ikiwa sio line:
                endelea
            fields = line.split()
            name = fields[0]
            value = len(tok_names)
            ikiwa name == 'ERRORTOKEN':
                ERRORTOKEN = value
            string = fields[1] ikiwa len(fields) > 1 isipokua Tupu
            ikiwa string:
                string = eval(string)
                string_to_tok[string] = value
            tok_names.append(name)
    rudisha tok_names, ERRORTOKEN, string_to_tok


eleza update_file(file, content):
    jaribu:
        ukijumuisha open(file, 'r') kama fobj:
            ikiwa fobj.read() == content:
                rudisha Uongo
    tatizo (OSError, ValueError):
        pita
    ukijumuisha open(file, 'w') kama fobj:
        fobj.write(content)
    rudisha Kweli


token_h_template = """\
/* Auto-generated by Tools/scripts/generate_token.py */

/* Token types */
#ifneleza Py_LIMITED_API
#ifneleza Py_TOKEN_H
#define Py_TOKEN_H
#ifeleza __cplusplus
extern "C" {
#endif

#uneleza TILDE   /* Prevent clash of our definition ukijumuisha system macro. Ex AIX, ioctl.h */

%s\
#define N_TOKENS        %d
#define NT_OFFSET       %d

/* Special definitions kila cooperation ukijumuisha parser */

#define ISTERMINAL(x)           ((x) < NT_OFFSET)
#define ISNONTERMINAL(x)        ((x) >= NT_OFFSET)
#define ISEOF(x)                ((x) == ENDMARKER)


PyAPI_DATA(const char * const) _PyParser_TokenNames[]; /* Token names */
PyAPI_FUNC(int) PyToken_OneChar(int);
PyAPI_FUNC(int) PyToken_TwoChars(int, int);
PyAPI_FUNC(int) PyToken_ThreeChars(int, int, int);

#ifeleza __cplusplus
}
#endif
#endikiwa /* !Py_TOKEN_H */
#endikiwa /* Py_LIMITED_API */
"""

eleza make_h(infile, outfile='Include/token.h'):
    tok_names, ERRORTOKEN, string_to_tok = load_tokens(infile)

    defines = []
    kila value, name kwenye enumerate(tok_names[:ERRORTOKEN + 1]):
        defines.append("#define %-15s %d\n" % (name, value))

    ikiwa update_file(outfile, token_h_template % (
            ''.join(defines),
            len(tok_names),
            NT_OFFSET
        )):
        andika("%s regenerated kutoka %s" % (outfile, infile))


token_c_template = """\
/* Auto-generated by Tools/scripts/generate_token.py */

#include "Python.h"
#include "token.h"

/* Token names */

const char * const _PyParser_TokenNames[] = {
%s\
};

/* Return the token corresponding to a single character */

int
PyToken_OneChar(int c1)
{
%s\
    rudisha OP;
}

int
PyToken_TwoChars(int c1, int c2)
{
%s\
    rudisha OP;
}

int
PyToken_ThreeChars(int c1, int c2, int c3)
{
%s\
    rudisha OP;
}
"""

eleza generate_chars_to_token(mapping, n=1):
    result = []
    write = result.append
    indent = '    ' * n
    write(indent)
    write('switch (c%d) {\n' % (n,))
    kila c kwenye sorted(mapping):
        write(indent)
        value = mapping[c]
        ikiwa isinstance(value, dict):
            write("case '%s':\n" % (c,))
            write(generate_chars_to_token(value, n + 1))
            write(indent)
            write('    koma;\n')
        isipokua:
            write("case '%s': rudisha %s;\n" % (c, value))
    write(indent)
    write('}\n')
    rudisha ''.join(result)

eleza make_c(infile, outfile='Parser/token.c'):
    tok_names, ERRORTOKEN, string_to_tok = load_tokens(infile)
    string_to_tok['<>'] = string_to_tok['!=']
    chars_to_token = {}
    kila string, value kwenye string_to_tok.items():
        assert 1 <= len(string) <= 3
        name = tok_names[value]
        m = chars_to_token.setdefault(len(string), {})
        kila c kwenye string[:-1]:
            m = m.setdefault(c, {})
        m[string[-1]] = name

    names = []
    kila value, name kwenye enumerate(tok_names):
        ikiwa value >= ERRORTOKEN:
            name = '<%s>' % name
        names.append('    "%s",\n' % name)
    names.append('    "<N_TOKENS>",\n')

    ikiwa update_file(outfile, token_c_template % (
            ''.join(names),
            generate_chars_to_token(chars_to_token[1]),
            generate_chars_to_token(chars_to_token[2]),
            generate_chars_to_token(chars_to_token[3])
        )):
        andika("%s regenerated kutoka %s" % (outfile, infile))


token_inc_template = """\
.. Auto-generated by Tools/scripts/generate_token.py
%s
.. data:: N_TOKENS

.. data:: NT_OFFSET
"""

eleza make_rst(infile, outfile='Doc/library/token-list.inc'):
    tok_names, ERRORTOKEN, string_to_tok = load_tokens(infile)
    tok_to_string = {value: s kila s, value kwenye string_to_tok.items()}

    names = []
    kila value, name kwenye enumerate(tok_names[:ERRORTOKEN + 1]):
        names.append('.. data:: %s' % (name,))
        ikiwa value kwenye tok_to_string:
            names.append('')
            names.append('   Token value kila ``"%s"``.' % tok_to_string[value])
        names.append('')

    ikiwa update_file(outfile, token_inc_template % '\n'.join(names)):
        andika("%s regenerated kutoka %s" % (outfile, infile))


token_py_template = '''\
"""Token constants."""
# Auto-generated by Tools/scripts/generate_token.py

__all__ = ['tok_name', 'ISTERMINAL', 'ISNONTERMINAL', 'ISEOF']

%s
N_TOKENS = %d
# Special definitions kila cooperation ukijumuisha parser
NT_OFFSET = %d

tok_name = {value: name
            kila name, value kwenye globals().items()
            ikiwa isinstance(value, int) na sio name.startswith('_')}
__all__.extend(tok_name.values())

EXACT_TOKEN_TYPES = {
%s
}

eleza ISTERMINAL(x):
    rudisha x < NT_OFFSET

eleza ISNONTERMINAL(x):
    rudisha x >= NT_OFFSET

eleza ISEOF(x):
    rudisha x == ENDMARKER
'''

eleza make_py(infile, outfile='Lib/token.py'):
    tok_names, ERRORTOKEN, string_to_tok = load_tokens(infile)

    constants = []
    kila value, name kwenye enumerate(tok_names):
        constants.append('%s = %d' % (name, value))
    constants.insert(ERRORTOKEN,
        "# These aren't used by the C tokenizer but are needed kila tokenize.py")

    token_types = []
    kila s, value kwenye sorted(string_to_tok.items()):
        token_types.append('    %r: %s,' % (s, tok_names[value]))

    ikiwa update_file(outfile, token_py_template % (
            '\n'.join(constants),
            len(tok_names),
            NT_OFFSET,
            '\n'.join(token_types),
        )):
        andika("%s regenerated kutoka %s" % (outfile, infile))


eleza main(op, infile='Grammar/Tokens', *args):
    make = globals()['make_' + op]
    make(infile, *args)


ikiwa __name__ == '__main__':
    agiza sys
    main(*sys.argv[1:])
